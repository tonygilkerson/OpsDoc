{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to OpsDoc\n\n\nHere you will find my personal Ops related notes.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-opsdoc",
            "text": "Here you will find my personal Ops related notes.",
            "title": "Welcome to OpsDoc"
        },
        {
            "location": "/cicd/",
            "text": "Introduction\n\n\nThis sections contains the notes taken while reading \nThe DevOps 2.4 Toolkit\n.  References to page numbers are assumed to be from this book unless otherwise specified.\n\n\n\n\nCreate a cluster\n\n\nDocker for Windows\n\n\nFollow \nthe docker-for-windows doc\n to create a cluster on a windows workstation.\n\n\nKubernetes dashboard\n\n\nThe full instructions can be found on the \nKubernetes dashboard readme page\n, in short, run the following:\n\n\nkubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml\n\n\n\n\nAccessing the dashboard UI\n\n\nRun \nkubectl proxy\n then open \nhttp://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/\n in your browser.\n\n\nTo log onto the UI you will need a token. Do the following to get a valid token:\n\n\nSECRET_NAME=$(kubectl -n kube-system \\\nget serviceAccount kubernetes-dashboard \\\n-o  jsonpath='{.secrets[0].name}')\n\nkubectl -n kube-system describe secrets/$SECRET_NAME\n# copy token from the output and use it to log onto the dashboard\n\n\n\n\nFor more informatoin on how to configure access see the following:\n\n\n\n\nAccessing the dashboard UI\n\n\nConfigure Service Account\n\n\n\n\nIngress Install\n\n\nI got the ingress to work fine on docker-for-win by following the \ninstall instructions for mac\n, in short, run the following:\n\n\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/cloud-generic.yaml\n\n\n\n\n\n\nDefine Some Environment Variables\n\n\nTo make live easy let set some environment variables for our shell:\n\n\nLB_IP\n\n\nFind your loadBalancer IP:\n\n\nOn docker-for-windows this is \n127.0.0.1\n  for other cluster you can try the following:\n\n\nkubectl -n ingress-nginx get svc ingress-nginx -o jsonpath=\"{.status.loadBalancer.ingress[0].hostname}\"\n#or\nkubectl -n ingress-nginx get svc ingress-nginx -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\"\n\n\n# set it\nLB_IP=\"127.0.0.1\"\n\n# or make it stick do this\necho \"export LB_IP=127.0.0.1\" >> ~/.profile\n. ~/.profile\n\n\n\n\nADDR\n\n\nADDR=$LB_IP.nip.io\necho $ADDR\nADDR_ESC=$(echo $ADDR | sed -e \"s@\\.@\\\\\\.@g\")\necho $ADDR_ESC\n\n# or make it stick do this\necho \"export ADDR=$LB_IP.nip.io\" >> ~/.profile\necho 'ADDR_ESC=$(echo $ADDR | sed -e \"s@\\.@\\\\\\.@g\")' >> ~/.profile\n. ~/.profile\n\n\n\n\nDH_USER\n\n\nDocker Hub user\n\n\nDH_USER=\"tonygilkerson\"\necho \"export DH_USER=tonygilkerson\" >> ~/.profile\n. ~/.profile\n\necho $DH_USER\n\n\n\n\n\n\nClient Tools\n\n\nkubectl\n\n\nFollow the \nInstall and Setup kubectl\n instructions on the official kubernetes site.\n\n\nhelm\n\n\nCurrently helm v2 needs tiller, when we get to v3 tiller will go away.  To install tiller:\n\n\ncd k8s-spec\n\n# Create service account\nkubectl create \\\n  -f helm/tiller-rbac.yml\n\n# Install server side tiller\nhelm init --service-account tiller\n\n# verify\nkubectl -n kube-system \\\n  rollout status deploy tiller-deploy\n\n\n\n\n\n\n\nChart Museum\n\n\nInstall Chart\n\n\nInstall \nstable/chartmuseum\n chart with override values (p. 130).\n\n\ncd kube-spec\n\n# set chartmuseum host env vars\necho $LB_IP\nCM_ADDR=\"cm.$LB_IP.nip.io\"\nCM_ADDR_ESC=$(echo $CM_ADDR | sed -e \"s@\\.@\\\\\\.@g\")\necho $CM_ADDR_ESC\n\n# install chart\nhelm install stable/chartmuseum \\\n--namespace charts \\\n--name cm \\\n--values helm/chartmuseum-values.yml \\\n--set ingress.hosts.\"$CM_ADDR_ESC\"={\"/\"} \\\n--set env.secret.BASIC_AUTH_USER=admin \\\n--set env.secret.BASIC_AUTH_PASS=admin\n\n# verify\nkubectl -n charts \\\nrollout status deploy \\\ncm-chartmuseum\n\ncurl \"http://$CM_ADDR/health\"  \ncurl \"http://$CM_ADDR/index.yaml\" -u admin:admin\n\n\n\n\n\nAdd to helm client\n\n\nAdd the chartmuseum repo to the helm client, then install a plugin that will allow us to push charts to the repo via helm CLI.\n\n\nhelm repo add chartmuseum http://$CM_ADDR --username admin --password admin\nhelm plugin install https://github.com/chartmuseum/helm-push\n\n# example push\nhelm push /path/to/chart chartmuseum --username admin --password admin\n\n# we can also do this\nhelm search chartmuseum/\nhelm repo update\nhelm search chartmuseum/\nhelm inspect chartmuseum/<my-chart>\n\n\n\n\n\nMore useful command can be found at \nvfarcic gist\n\n\nUpgrade\n\n\nTo upgrade after modifying values.\n\n\n helm upgrade cm stable/chartmuseum \\\n  --values helm/chartmuseum-values.yml \\\n  --set ingress.hosts.\"$CM_ADDR_ESC\"={\"/\"} \\\n  --set env.secret.BASIC_AUTH_USER=admin \\\n  --set env.secret.BASIC_AUTH_PASS=admin\n\n\n\n\n\nAccessing the UI\n\n\nTo access chartmuseum from my laptop I use \n$CM_ADDR\n as seen above however this is not valid in a pipeline build container. From within a container use the \nhttp://[SERVICE_NAME].[NAMESPACE]\n format as seen here:\n\n\n# from within the cluster, pod to pod\nhttp://cm-chartmuseum.charts:8080\n\n# from my browser\nhttp://cm.127.0.0.1.nip.io\n\n\n\n\n\n\nJenkins\n\n\nInstall\n\n\nInstall \nstable/jenkins\n chart with override values (p. 130).\n\n\ncd kube-spec\n\n# set jenkins host env vars\necho $LB_IP\nJENKINS_ADDR=\"jenkins.$LB_IP.nip.io\"\necho $JENKINS_ADDR\n\n# install jenkins (p. 135 and p. 179)\nhelm install stable/jenkins \\\n--name jenkins \\\n--namespace jenkins \\\n--values helm/jenkins-values.yml \\\n--set Master.HostName=$JENKINS_ADDR\n\n# verify\nkubectl -n jenkins rollout status deployment jenkins\n\n\n\n\nopen \nhttp://$JENKINS_ADDR/configure\n (i.e. http://jenkins.127.0.0.1.nip.io) use the \nJENKINS_PASS\n to logon as \nadmin\n.\n\n\nJENKINS_PASS=$(kubectl -n jenkins get secret jenkins \\\n-o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode)\n\necho $JENKINS_PASS\n\n\n\n\nNamespace URLs\n\n\nTo allow communication between the Jenkins master and the slave nodes in other namespaces open \nhttp://$JENKINS_ADDR/configure\n and change the \nhttp://[NAMESPACE]\n format to the \nhttp://[SERVICE_NAME].[NAMESPACE]\n as shown in the following table.\n\n\nCloud->Kubernetes Section:\n\n\n\n\n\n\n\n\nlabel\n\n\nold values\n\n\nupdated value\n\n\n\n\n\n\n\n\n\n\nJenkins URL\n\n\nhttp://jenkins:8080\n\n\nhttp://jenkins.jenkins:8080\n\n\n\n\n\n\nJenkins tunnel\n\n\njenkins-agent:50000\n\n\njenkins-agent.jenkins:50000\n\n\n\n\n\n\n\n\nTiller\n\n\nNote:\n We have Tiller running in the \nkube-system\n Namespace. However, our agent Pods running in \n<app-namespace>\n do not have permissions to access it. We could extend the permissions, but that would allow the pods in that Namespace to gain almost complete control over the whole cluster. Unless your organization is very small, that is often not acceptable. Instead, we\u2019ll deploy another Tiller instance in the \n<app-namespace>\n namespace and tie it to the \nbuild\n ServiceAccount. That will give the new tiller the same permissions in the \n<app-namespace>\n namespace. It\u2019ll be able to do anything in those, but nothing anywhere else (p. 195).\n\n\nhelm init --service-account build --tiller-namespace <app-namespace>\n\n\n\n\nRBAC\n\n\nThis next section I am not so sure about.  Docker-for-windows does not seem to use the rbac, so I will have to validate the following in a real cluster.\n\n\nAdd rbac for jenkins builds.\n\n\nkubectl apply -f k8s/ns.yml\n\n\n\n\nGlobal Pipeline Libraries\n\n\nOpen \nhttp://jenkins.$ADDR/configure\n\n\nSearch for \nGlobal Pipeline Libraries\n section of the configuration, and click the Add button. Type \nmy-library\n as the Name (it can be anything else) and \nmaster\n as the Default version. In our context, the latter defines the branch from which we\u2019ll load the libraries.\n\n\nNext, we\u2019ll click the \nLoad implicitly\n checkbox. As a result, the libraries will be available automatically to all the pipeline jobs. Otherwise, our jobs would need to have @Library('my-library') instruction. Select \nModern SCM\n from the Retrieval method section and select \nGit\n from \nSource Code Management\n and then specify the repository from which Jenkins will load the libraries \nhttps://github.com/tgilkerson/jenkins-shared-libraries.git\n\n\nDon\u2019t forget to click the Save button to persist the changes!\n\n\nGlobal credentials\n\n\nSelect \nCredentials\n and in the \nglobal\n store for the \nJenkins\n scope, then click to \nAdd Credentials\n\n\nDocker Hub\n\n\n scope: Global\n\n Username: tonygilkerson\n\n Password: \nmypwd\n\n\n ID: docker\n\n\nChart Museum\n\n\n scope: Global\n\n Username: admin\n\n Password: admin\n\n ID: chartmuseum\n\n\nUpgrade Jenkins\n\n\nTo upgrade after modifying values in \nhelm/jenkins-values.yml\n (pg126)\n\n\n helm upgrade jenkins stable/jenkins \\\n --values helm/jenkins-values.yml \\\n --set Master.HostName=$HOST \\\n --reuse-values\n\n\n\n\n\n\n\nPrepare Apps\n\n\ngo-demo-5\n\n\ncd go-demo-5\nkubectl apply -f k8s/build.yml\n\n# do once then commit changes\ncat Jenkinsfile.orig \\\n| sed -e \"s@acme.com@$ADDR@g\" \\\n| sed -e \"s@vfarcic@$DH_USER@g\" \\\n| tee Jenkinsfile\n\n# do once then commit changes\ncat helm/go-demo-5/deployment-orig.yaml \\\n| sed -e \"s@vfarcic@$DH_USER@g\" \\\n| tee helm/go-demo-5/templates/deployment.yaml\n\ngit add .\ngit commit -m \"set docker hub user\"\ngit push\n\n\n\n\nInstall Tiller for this app\n\n\nhelm init --service-account build \\\n--tiller-namespace go-demo-5-build\n\n\n\n\nCreate Jenkins Cloud for this App\n\n\nIf you look in the Jenkinsfile you will see a reference to \ngo-demo5-build\n.  \n\n\ncd go-demo-5\ncat Jenkinsfile\n\n...\n  agent {\n    kubernetes {\n      cloud \"go-demo-5-build\"\n      label \"go-demo-5-build\"\n      serviceAccount \"build\"\n      yamlFile \"KubernetesPod.yaml\"\n    }\n...\n\n\n\n\n\nTo configure the cloud open the Jenkins UI and navigate to the \n/configure\n page.\n\n\nPlease scroll to the bottom of the page, expand the \nAdd a new cloud\n list, and select \nKubernetes\n. A new set of fields will appear. Type \ngo-demo-5-build\n as the name. It matches the cloud entry inside kubernetes block of our pipeline. Next, type \ngo-demo-5-build\n as the Kubernetes Namespace. Just as with the other Kubernetes Cloud that was already defined in our Jenkins instance, the value of the Jenkins URL should be \nhttp://prod-jenkins.prod:8080\n, and the Jenkins tunnel should be set to \nprod-jenkins-agent.prod:50000\n.\n\n\nSandbox\n\n\nTo test a release:\n\n\nSANDBOX_ADDR=\"go-demo-5-sandbox.$ADDR\"\n\nhelm install helm/go-demo-5 -n go-demo-5-sandbox --namespace go-demo-5-sandbox\n\n# when done\nhelm delete go-demo-5-sandbox --purge\n\n\n\n\nTODO add verification steps\n\n\n\n\naegjxnode\n\n\ncd aegjxnode\nkubectl apply -f k8s/build.yml\n\n# edit Jenkinsfile and set correct values for the environment section\n...\nenvironment {\n  image = \"tonygilkerson/aegjxnode\"\n  project = \"aegjxnode-build\"\n  domain = \"127.0.0.1.nip.io\"\n  cmAddr = \"cm-chartmuseum.charts:8080\"\n}\n...\n\n# make it stick\ngit add .\ngit commit -m \"set docker hub user\"\ngit push\n\n\n\n\nInstall Tiller for this app\n\n\nhelm init --service-account build \\\n--tiller-namespace aegjxnode-build\n\n\n\n\nCreate Jenkins Cloud for this App\n\n\nIf you look in the Jenkinsfile you will see a reference to \naegjxnode-build\n.  \n\n\ncd aegjxnode\ncat Jenkinsfile\n\n...\n  agent {\n    kubernetes {\n      cloud \"aegjxnode-build\"\n      label \"aegjxnode-build\"\n      serviceAccount \"build\"\n      yamlFile \"KubernetesPod.yaml\"\n    }\n...\n\n\n\n\n\nTo configure the cloud open the Jenkins UI and navigate to the \n/configure\n page.\n\n\nPlease scroll to the bottom of the page, expand the \nAdd a new cloud\n list, and select \nKubernetes\n. A new set of fields will appear. Type \naegjxnode-build\n as the name. It matches the cloud entry inside kubernetes block of our pipeline. Next, type \naegjxnode-build\n as the Kubernetes Namespace. Just as with the other Kubernetes Cloud that was already defined in our Jenkins instance, the value of the Jenkins URL should be \nhttp://prod-jenkins.prod:8080\n, and the Jenkins tunnel should be set to \nprod-jenkins-agent.prod:50000\n.\n\n\nSandbox\n\n\nTo test a release:\n\n\nTODO - refer to step above once they quit changing\n\n\n\n\n\nTODO add verification steps\n\n\n\n\nDeploy Environment\n\n\nOne or more apps are owned by a team.  Each team has a production environment such as \nteam1-pord\n.  The chart in the \nhelm\n folder will reference all the apps that are required for this environment in \nrequirements.yaml\n\n\nManual Deploy\n\n\nClone the environment chart and review the values.\n\n\ngit clone https://github.com/tgilkerson/team1-prod.git\ncd team1-prod\nls -l helm\n\n\n\n\nVerify the host address is correct in \nhelm/values.yaml\n.\n\n\ngo-demo-5:\n  ingress:\n    host: go-demo-5.127.0.0.1.nip.io\naegjxnode:\n  ingress:\n    host: aegjxnode.127.0.0.1.nip.io\n\n\n\n\nLater on, we\u2019ll use Jenkins to install (or upgrade) the Chart, so we should push the changes to GitHub.\n\n\ngit add .\ngit commit -m \"Fix the address\"\ngit push\n\n\n\n\nAll Helm dependencies need to be downloaded to the charts directory before they are installed. We\u2019ll do that through the helm dependency update command. Don\u2019t worry if some of the repositories are not reachable. You might see messages stating that Helm was unable to get an update from \nlocal\n or \nchartmuseum\n repositories.\n\n\nFirst lets do a manually deploy of the \nteam1-prod\n environment.\n\n\ncd team1-prod\n\nhelm dependency update helm\nls helm/charts/\n\n# To confirm\nls -l helm/charts\n\n# The output should look like this\naegjxnode-0.1.9.tgz\ngo-demo-5-0.0.1.tgz\n\n# The first time\nhelm install helm -n team1-prod --namespace team1-prod\n\n# Or do this the -i will install if it does not exist\nhelm upgrade -i team1-prod helm --namespace team1-prod\n\n# verify\nkubectl -n team1-prod rollout status deploy team1-prod-go-demo-5\ncurl go-demo-5.127.0.0.1.nip.io/demo/hello\n\n# clean up\nhelm list\nhelm delete team1-prod --purge\n\n\n\n\n\nAuto Deploy\n\n\nThe CI/CD pipeline will automate the manual steps above.\n\n\n\n\nJeninsX\n\n\nJenkinsX was not in the book, it is too new, however I did look into it a bit to take a look at their workflow.\n\n\nTo install on my docker-for-windows cluster:\n\n\nTaken from \ngetting started\n\n\njx install --provider=kubernetes --on-premise\n\nJenkins X installation completed successfully\nYour admin password is: =rvC9qvz0fq9is^Z6SwY\n\n\n--------------------------------------------------------------------------------\n\n# Utility\n\nSome utility functions and other useful stuff to help debug.\n\n```bash\nkubectl -n <target-namespace> run -it \\\n--image ubuntu aegutil --restart=Never --rm sh\n\n# then you can...\napt-get update\napt-get install curl\napt-get install iputils-ping",
            "title": "CI/CD"
        },
        {
            "location": "/cicd/#introduction",
            "text": "This sections contains the notes taken while reading  The DevOps 2.4 Toolkit .  References to page numbers are assumed to be from this book unless otherwise specified.",
            "title": "Introduction"
        },
        {
            "location": "/cicd/#create-a-cluster",
            "text": "",
            "title": "Create a cluster"
        },
        {
            "location": "/cicd/#docker-for-windows",
            "text": "Follow  the docker-for-windows doc  to create a cluster on a windows workstation.",
            "title": "Docker for Windows"
        },
        {
            "location": "/cicd/#kubernetes-dashboard",
            "text": "The full instructions can be found on the  Kubernetes dashboard readme page , in short, run the following:  kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml",
            "title": "Kubernetes dashboard"
        },
        {
            "location": "/cicd/#accessing-the-dashboard-ui",
            "text": "Run  kubectl proxy  then open  http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/  in your browser.  To log onto the UI you will need a token. Do the following to get a valid token:  SECRET_NAME=$(kubectl -n kube-system \\\nget serviceAccount kubernetes-dashboard \\\n-o  jsonpath='{.secrets[0].name}')\n\nkubectl -n kube-system describe secrets/$SECRET_NAME\n# copy token from the output and use it to log onto the dashboard  For more informatoin on how to configure access see the following:   Accessing the dashboard UI  Configure Service Account",
            "title": "Accessing the dashboard UI"
        },
        {
            "location": "/cicd/#ingress-install",
            "text": "I got the ingress to work fine on docker-for-win by following the  install instructions for mac , in short, run the following:  kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/cloud-generic.yaml",
            "title": "Ingress Install"
        },
        {
            "location": "/cicd/#define-some-environment-variables",
            "text": "To make live easy let set some environment variables for our shell:  LB_IP  Find your loadBalancer IP:  On docker-for-windows this is  127.0.0.1   for other cluster you can try the following:  kubectl -n ingress-nginx get svc ingress-nginx -o jsonpath=\"{.status.loadBalancer.ingress[0].hostname}\"\n#or\nkubectl -n ingress-nginx get svc ingress-nginx -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\"\n\n\n# set it\nLB_IP=\"127.0.0.1\"\n\n# or make it stick do this\necho \"export LB_IP=127.0.0.1\" >> ~/.profile\n. ~/.profile  ADDR  ADDR=$LB_IP.nip.io\necho $ADDR\nADDR_ESC=$(echo $ADDR | sed -e \"s@\\.@\\\\\\.@g\")\necho $ADDR_ESC\n\n# or make it stick do this\necho \"export ADDR=$LB_IP.nip.io\" >> ~/.profile\necho 'ADDR_ESC=$(echo $ADDR | sed -e \"s@\\.@\\\\\\.@g\")' >> ~/.profile\n. ~/.profile  DH_USER  Docker Hub user  DH_USER=\"tonygilkerson\"\necho \"export DH_USER=tonygilkerson\" >> ~/.profile\n. ~/.profile\n\necho $DH_USER",
            "title": "Define Some Environment Variables"
        },
        {
            "location": "/cicd/#client-tools",
            "text": "",
            "title": "Client Tools"
        },
        {
            "location": "/cicd/#kubectl",
            "text": "Follow the  Install and Setup kubectl  instructions on the official kubernetes site.",
            "title": "kubectl"
        },
        {
            "location": "/cicd/#helm",
            "text": "Currently helm v2 needs tiller, when we get to v3 tiller will go away.  To install tiller:  cd k8s-spec\n\n# Create service account\nkubectl create \\\n  -f helm/tiller-rbac.yml\n\n# Install server side tiller\nhelm init --service-account tiller\n\n# verify\nkubectl -n kube-system \\\n  rollout status deploy tiller-deploy",
            "title": "helm"
        },
        {
            "location": "/cicd/#chart-museum",
            "text": "",
            "title": "Chart Museum"
        },
        {
            "location": "/cicd/#install-chart",
            "text": "Install  stable/chartmuseum  chart with override values (p. 130).  cd kube-spec\n\n# set chartmuseum host env vars\necho $LB_IP\nCM_ADDR=\"cm.$LB_IP.nip.io\"\nCM_ADDR_ESC=$(echo $CM_ADDR | sed -e \"s@\\.@\\\\\\.@g\")\necho $CM_ADDR_ESC\n\n# install chart\nhelm install stable/chartmuseum \\\n--namespace charts \\\n--name cm \\\n--values helm/chartmuseum-values.yml \\\n--set ingress.hosts.\"$CM_ADDR_ESC\"={\"/\"} \\\n--set env.secret.BASIC_AUTH_USER=admin \\\n--set env.secret.BASIC_AUTH_PASS=admin\n\n# verify\nkubectl -n charts \\\nrollout status deploy \\\ncm-chartmuseum\n\ncurl \"http://$CM_ADDR/health\"  \ncurl \"http://$CM_ADDR/index.yaml\" -u admin:admin",
            "title": "Install Chart"
        },
        {
            "location": "/cicd/#add-to-helm-client",
            "text": "Add the chartmuseum repo to the helm client, then install a plugin that will allow us to push charts to the repo via helm CLI.  helm repo add chartmuseum http://$CM_ADDR --username admin --password admin\nhelm plugin install https://github.com/chartmuseum/helm-push\n\n# example push\nhelm push /path/to/chart chartmuseum --username admin --password admin\n\n# we can also do this\nhelm search chartmuseum/\nhelm repo update\nhelm search chartmuseum/\nhelm inspect chartmuseum/<my-chart>  More useful command can be found at  vfarcic gist",
            "title": "Add to helm client"
        },
        {
            "location": "/cicd/#upgrade",
            "text": "To upgrade after modifying values.   helm upgrade cm stable/chartmuseum \\\n  --values helm/chartmuseum-values.yml \\\n  --set ingress.hosts.\"$CM_ADDR_ESC\"={\"/\"} \\\n  --set env.secret.BASIC_AUTH_USER=admin \\\n  --set env.secret.BASIC_AUTH_PASS=admin",
            "title": "Upgrade"
        },
        {
            "location": "/cicd/#accessing-the-ui",
            "text": "To access chartmuseum from my laptop I use  $CM_ADDR  as seen above however this is not valid in a pipeline build container. From within a container use the  http://[SERVICE_NAME].[NAMESPACE]  format as seen here:  # from within the cluster, pod to pod\nhttp://cm-chartmuseum.charts:8080\n\n# from my browser\nhttp://cm.127.0.0.1.nip.io",
            "title": "Accessing the UI"
        },
        {
            "location": "/cicd/#jenkins",
            "text": "",
            "title": "Jenkins"
        },
        {
            "location": "/cicd/#install",
            "text": "Install  stable/jenkins  chart with override values (p. 130).  cd kube-spec\n\n# set jenkins host env vars\necho $LB_IP\nJENKINS_ADDR=\"jenkins.$LB_IP.nip.io\"\necho $JENKINS_ADDR\n\n# install jenkins (p. 135 and p. 179)\nhelm install stable/jenkins \\\n--name jenkins \\\n--namespace jenkins \\\n--values helm/jenkins-values.yml \\\n--set Master.HostName=$JENKINS_ADDR\n\n# verify\nkubectl -n jenkins rollout status deployment jenkins  open  http://$JENKINS_ADDR/configure  (i.e. http://jenkins.127.0.0.1.nip.io) use the  JENKINS_PASS  to logon as  admin .  JENKINS_PASS=$(kubectl -n jenkins get secret jenkins \\\n-o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode)\n\necho $JENKINS_PASS",
            "title": "Install"
        },
        {
            "location": "/cicd/#namespace-urls",
            "text": "To allow communication between the Jenkins master and the slave nodes in other namespaces open  http://$JENKINS_ADDR/configure  and change the  http://[NAMESPACE]  format to the  http://[SERVICE_NAME].[NAMESPACE]  as shown in the following table.  Cloud->Kubernetes Section:     label  old values  updated value      Jenkins URL  http://jenkins:8080  http://jenkins.jenkins:8080    Jenkins tunnel  jenkins-agent:50000  jenkins-agent.jenkins:50000",
            "title": "Namespace URLs"
        },
        {
            "location": "/cicd/#tiller",
            "text": "Note:  We have Tiller running in the  kube-system  Namespace. However, our agent Pods running in  <app-namespace>  do not have permissions to access it. We could extend the permissions, but that would allow the pods in that Namespace to gain almost complete control over the whole cluster. Unless your organization is very small, that is often not acceptable. Instead, we\u2019ll deploy another Tiller instance in the  <app-namespace>  namespace and tie it to the  build  ServiceAccount. That will give the new tiller the same permissions in the  <app-namespace>  namespace. It\u2019ll be able to do anything in those, but nothing anywhere else (p. 195).  helm init --service-account build --tiller-namespace <app-namespace>",
            "title": "Tiller"
        },
        {
            "location": "/cicd/#rbac",
            "text": "This next section I am not so sure about.  Docker-for-windows does not seem to use the rbac, so I will have to validate the following in a real cluster.  Add rbac for jenkins builds.  kubectl apply -f k8s/ns.yml",
            "title": "RBAC"
        },
        {
            "location": "/cicd/#global-pipeline-libraries",
            "text": "Open  http://jenkins.$ADDR/configure  Search for  Global Pipeline Libraries  section of the configuration, and click the Add button. Type  my-library  as the Name (it can be anything else) and  master  as the Default version. In our context, the latter defines the branch from which we\u2019ll load the libraries.  Next, we\u2019ll click the  Load implicitly  checkbox. As a result, the libraries will be available automatically to all the pipeline jobs. Otherwise, our jobs would need to have @Library('my-library') instruction. Select  Modern SCM  from the Retrieval method section and select  Git  from  Source Code Management  and then specify the repository from which Jenkins will load the libraries  https://github.com/tgilkerson/jenkins-shared-libraries.git  Don\u2019t forget to click the Save button to persist the changes!",
            "title": "Global Pipeline Libraries"
        },
        {
            "location": "/cicd/#global-credentials",
            "text": "Select  Credentials  and in the  global  store for the  Jenkins  scope, then click to  Add Credentials  Docker Hub   scope: Global  Username: tonygilkerson  Password:  mypwd   ID: docker  Chart Museum   scope: Global  Username: admin  Password: admin  ID: chartmuseum",
            "title": "Global credentials"
        },
        {
            "location": "/cicd/#upgrade-jenkins",
            "text": "To upgrade after modifying values in  helm/jenkins-values.yml  (pg126)   helm upgrade jenkins stable/jenkins \\\n --values helm/jenkins-values.yml \\\n --set Master.HostName=$HOST \\\n --reuse-values",
            "title": "Upgrade Jenkins"
        },
        {
            "location": "/cicd/#prepare-apps",
            "text": "",
            "title": "Prepare Apps"
        },
        {
            "location": "/cicd/#go-demo-5",
            "text": "cd go-demo-5\nkubectl apply -f k8s/build.yml\n\n# do once then commit changes\ncat Jenkinsfile.orig \\\n| sed -e \"s@acme.com@$ADDR@g\" \\\n| sed -e \"s@vfarcic@$DH_USER@g\" \\\n| tee Jenkinsfile\n\n# do once then commit changes\ncat helm/go-demo-5/deployment-orig.yaml \\\n| sed -e \"s@vfarcic@$DH_USER@g\" \\\n| tee helm/go-demo-5/templates/deployment.yaml\n\ngit add .\ngit commit -m \"set docker hub user\"\ngit push  Install Tiller for this app  helm init --service-account build \\\n--tiller-namespace go-demo-5-build  Create Jenkins Cloud for this App  If you look in the Jenkinsfile you will see a reference to  go-demo5-build .    cd go-demo-5\ncat Jenkinsfile\n\n...\n  agent {\n    kubernetes {\n      cloud \"go-demo-5-build\"\n      label \"go-demo-5-build\"\n      serviceAccount \"build\"\n      yamlFile \"KubernetesPod.yaml\"\n    }\n...  To configure the cloud open the Jenkins UI and navigate to the  /configure  page.  Please scroll to the bottom of the page, expand the  Add a new cloud  list, and select  Kubernetes . A new set of fields will appear. Type  go-demo-5-build  as the name. It matches the cloud entry inside kubernetes block of our pipeline. Next, type  go-demo-5-build  as the Kubernetes Namespace. Just as with the other Kubernetes Cloud that was already defined in our Jenkins instance, the value of the Jenkins URL should be  http://prod-jenkins.prod:8080 , and the Jenkins tunnel should be set to  prod-jenkins-agent.prod:50000 .  Sandbox  To test a release:  SANDBOX_ADDR=\"go-demo-5-sandbox.$ADDR\"\n\nhelm install helm/go-demo-5 -n go-demo-5-sandbox --namespace go-demo-5-sandbox\n\n# when done\nhelm delete go-demo-5-sandbox --purge  TODO add verification steps",
            "title": "go-demo-5"
        },
        {
            "location": "/cicd/#aegjxnode",
            "text": "cd aegjxnode\nkubectl apply -f k8s/build.yml\n\n# edit Jenkinsfile and set correct values for the environment section\n...\nenvironment {\n  image = \"tonygilkerson/aegjxnode\"\n  project = \"aegjxnode-build\"\n  domain = \"127.0.0.1.nip.io\"\n  cmAddr = \"cm-chartmuseum.charts:8080\"\n}\n...\n\n# make it stick\ngit add .\ngit commit -m \"set docker hub user\"\ngit push  Install Tiller for this app  helm init --service-account build \\\n--tiller-namespace aegjxnode-build  Create Jenkins Cloud for this App  If you look in the Jenkinsfile you will see a reference to  aegjxnode-build .    cd aegjxnode\ncat Jenkinsfile\n\n...\n  agent {\n    kubernetes {\n      cloud \"aegjxnode-build\"\n      label \"aegjxnode-build\"\n      serviceAccount \"build\"\n      yamlFile \"KubernetesPod.yaml\"\n    }\n...  To configure the cloud open the Jenkins UI and navigate to the  /configure  page.  Please scroll to the bottom of the page, expand the  Add a new cloud  list, and select  Kubernetes . A new set of fields will appear. Type  aegjxnode-build  as the name. It matches the cloud entry inside kubernetes block of our pipeline. Next, type  aegjxnode-build  as the Kubernetes Namespace. Just as with the other Kubernetes Cloud that was already defined in our Jenkins instance, the value of the Jenkins URL should be  http://prod-jenkins.prod:8080 , and the Jenkins tunnel should be set to  prod-jenkins-agent.prod:50000 .  Sandbox  To test a release:  TODO - refer to step above once they quit changing  TODO add verification steps",
            "title": "aegjxnode"
        },
        {
            "location": "/cicd/#deploy-environment",
            "text": "One or more apps are owned by a team.  Each team has a production environment such as  team1-pord .  The chart in the  helm  folder will reference all the apps that are required for this environment in  requirements.yaml",
            "title": "Deploy Environment"
        },
        {
            "location": "/cicd/#manual-deploy",
            "text": "Clone the environment chart and review the values.  git clone https://github.com/tgilkerson/team1-prod.git\ncd team1-prod\nls -l helm  Verify the host address is correct in  helm/values.yaml .  go-demo-5:\n  ingress:\n    host: go-demo-5.127.0.0.1.nip.io\naegjxnode:\n  ingress:\n    host: aegjxnode.127.0.0.1.nip.io  Later on, we\u2019ll use Jenkins to install (or upgrade) the Chart, so we should push the changes to GitHub.  git add .\ngit commit -m \"Fix the address\"\ngit push  All Helm dependencies need to be downloaded to the charts directory before they are installed. We\u2019ll do that through the helm dependency update command. Don\u2019t worry if some of the repositories are not reachable. You might see messages stating that Helm was unable to get an update from  local  or  chartmuseum  repositories.  First lets do a manually deploy of the  team1-prod  environment.  cd team1-prod\n\nhelm dependency update helm\nls helm/charts/\n\n# To confirm\nls -l helm/charts\n\n# The output should look like this\naegjxnode-0.1.9.tgz\ngo-demo-5-0.0.1.tgz\n\n# The first time\nhelm install helm -n team1-prod --namespace team1-prod\n\n# Or do this the -i will install if it does not exist\nhelm upgrade -i team1-prod helm --namespace team1-prod\n\n# verify\nkubectl -n team1-prod rollout status deploy team1-prod-go-demo-5\ncurl go-demo-5.127.0.0.1.nip.io/demo/hello\n\n# clean up\nhelm list\nhelm delete team1-prod --purge  Auto Deploy  The CI/CD pipeline will automate the manual steps above.",
            "title": "Manual Deploy"
        },
        {
            "location": "/cicd/#jeninsx",
            "text": "JenkinsX was not in the book, it is too new, however I did look into it a bit to take a look at their workflow.  To install on my docker-for-windows cluster:  Taken from  getting started  jx install --provider=kubernetes --on-premise\n\nJenkins X installation completed successfully\nYour admin password is: =rvC9qvz0fq9is^Z6SwY\n\n\n--------------------------------------------------------------------------------\n\n# Utility\n\nSome utility functions and other useful stuff to help debug.\n\n```bash\nkubectl -n <target-namespace> run -it \\\n--image ubuntu aegutil --restart=Never --rm sh\n\n# then you can...\napt-get update\napt-get install curl\napt-get install iputils-ping",
            "title": "JeninsX"
        },
        {
            "location": "/about/",
            "text": "This is my personal notes Ops related stuff.",
            "title": "About"
        }
    ]
}