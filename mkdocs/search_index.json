{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to OpsDoc\n\n\nHere you will find my personal Ops related notes.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-opsdoc",
            "text": "Here you will find my personal Ops related notes.",
            "title": "Welcome to OpsDoc"
        },
        {
            "location": "/mon-log-scale/",
            "text": "Introduction\n\n\nThis section contains notes taken when reading through \nThe DevOps 2.5 Toolkit\n book.\n\n\nAt this time I am working through the \nCollecting and Querying Metrics and Sending Alerts\n chapter and don't have much to add beyond what is contained in  \nthis gist",
            "title": "Monitoring"
        },
        {
            "location": "/mon-log-scale/#introduction",
            "text": "This section contains notes taken when reading through  The DevOps 2.5 Toolkit  book.  At this time I am working through the  Collecting and Querying Metrics and Sending Alerts  chapter and don't have much to add beyond what is contained in   this gist",
            "title": "Introduction"
        },
        {
            "location": "/cicd/cicd-toolkit24/",
            "text": "Introduction\n\n\nThis sections contains the notes taken while reading \nThe DevOps 2.4 Toolkit\n.  References to page numbers are assumed to be from this book unless otherwise specified.\n\n\n\n\nCreate a cluster\n\n\nDocker for Windows\n\n\nFollow \nthe docker-for-windows doc\n to create a cluster on a windows workstation.\n\n\nKubernetes dashboard\n\n\nThe full instructions can be found on the \nKubernetes dashboard readme page\n, in short, run the following:\n\n\nkubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml\n\n\n\n\nAccessing the dashboard UI\n\n\nRun \nkubectl proxy\n then open \nhttp://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/\n in your browser.\n\n\nTo log onto the UI you will need a token. Do the following to get a valid token:\n\n\nSECRET_NAME=$(kubectl -n kube-system \\\nget serviceAccount kubernetes-dashboard \\\n-o  jsonpath='{.secrets[0].name}')\n\nkubectl -n kube-system describe secrets/$SECRET_NAME\n# copy token from the output and use it to log onto the dashboard\n\n\n\n\nFor more informatoin on how to configure access see the following:\n\n\n\n\nAccessing the dashboard UI\n\n\nConfigure Service Account\n\n\n\n\nIngress Install\n\n\nI got the ingress to work fine on docker-for-win by following the \ninstall instructions for mac\n, in short, run the following:\n\n\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/cloud-generic.yaml\n\n\n\n\n\n\nDefine Some Environment Variables\n\n\nTo make live easy let set some environment variables for our shell:\n\n\nLB_IP\n\n\nFind your loadBalancer IP:\n\n\nOn docker-for-windows this is \n127.0.0.1\n  for other cluster you can try the following:\n\n\nUpdate\n - I think it might be better to use the IP of my laptop i.e. \n192.168.1.164\n.  This might make is so the pod to pod communication works.  I am not sure.  The later 2.5 book started doing this.  I keep the notes here point to the loopback interface because that is what I used but if I go back through this a second time I should try my laptop IP and if all works out update this doc.\n\n\nkubectl -n ingress-nginx get svc ingress-nginx -o jsonpath=\"{.status.loadBalancer.ingress[0].hostname}\"\n#or\nkubectl -n ingress-nginx get svc ingress-nginx -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\"\n\n\n# set it\nLB_IP=\"127.0.0.1\"\n\n# or make it stick do this\necho \"export LB_IP=127.0.0.1\" >> ~/.profile\n. ~/.profile\n\n\n\n\nADDR\n\n\nADDR=$LB_IP.nip.io\necho $ADDR\nADDR_ESC=$(echo $ADDR | sed -e \"s@\\.@\\\\\\.@g\")\necho $ADDR_ESC\n\n# or make it stick do this\necho \"export ADDR=$LB_IP.nip.io\" >> ~/.profile\necho 'ADDR_ESC=$(echo $ADDR | sed -e \"s@\\.@\\\\\\.@g\")' >> ~/.profile\n. ~/.profile\n\n\n\n\nDH_USER\n\n\nDocker Hub user\n\n\nDH_USER=\"tonygilkerson\"\necho \"export DH_USER=tonygilkerson\" >> ~/.profile\n. ~/.profile\n\necho $DH_USER\n\n\n\n\n\n\nClient Tools\n\n\nkubectl\n\n\nFollow the \nInstall and Setup kubectl\n instructions on the official kubernetes site.\n\n\nhelm\n\n\nCurrently helm v2 needs tiller, when we get to v3 tiller will go away.  To install tiller:\n\n\ncd k8s-spec\n\n# Create service account\nkubectl create \\\n  -f helm/tiller-rbac.yml\n\n# Install server side tiller\nhelm init --service-account tiller\n\n# verify\nkubectl -n kube-system \\\n  rollout status deploy tiller-deploy\n\n\n\n\n\n\n\nChart Museum\n\n\nInstall Chart\n\n\nInstall \nstable/chartmuseum\n chart with override values (p. 130).\n\n\ncd kube-spec\n\n# set chartmuseum host env vars\necho $LB_IP\nCM_ADDR=\"cm.$LB_IP.nip.io\"\nCM_ADDR_ESC=$(echo $CM_ADDR | sed -e \"s@\\.@\\\\\\.@g\")\necho $CM_ADDR_ESC\n\n# install chart\nhelm install stable/chartmuseum \\\n--namespace charts \\\n--name cm \\\n--values helm/chartmuseum-values.yml \\\n--set ingress.hosts.\"$CM_ADDR_ESC\"={\"/\"} \\\n--set env.secret.BASIC_AUTH_USER=admin \\\n--set env.secret.BASIC_AUTH_PASS=admin\n\n# verify\nkubectl -n charts \\\nrollout status deploy \\\ncm-chartmuseum\n\ncurl \"http://$CM_ADDR/health\"  \ncurl \"http://$CM_ADDR/index.yaml\" -u admin:admin\n\n\n\n\n\nAdd to helm client\n\n\nAdd the chartmuseum repo to the helm client, then install a plugin that will allow us to push charts to the repo via helm CLI.\n\n\nhelm repo add chartmuseum http://$CM_ADDR --username admin --password admin\nhelm plugin install https://github.com/chartmuseum/helm-push\n\n# example push\nhelm push /path/to/chart chartmuseum --username admin --password admin\n\n# we can also do this\nhelm search chartmuseum/\nhelm repo update\nhelm search chartmuseum/\nhelm inspect chartmuseum/<my-chart>\n\n\n\n\n\nMore useful command can be found at \nvfarcic gist\n\n\nUpgrade\n\n\nTo upgrade after modifying values.\n\n\n helm upgrade cm stable/chartmuseum \\\n  --values helm/chartmuseum-values.yml \\\n  --set ingress.hosts.\"$CM_ADDR_ESC\"={\"/\"} \\\n  --set env.secret.BASIC_AUTH_USER=admin \\\n  --set env.secret.BASIC_AUTH_PASS=admin\n\n\n\n\n\nAccessing the UI\n\n\nTo access chartmuseum from my laptop I use \n$CM_ADDR\n as seen above however this is not valid in a pipeline build container. From within a container use the \nhttp://[SERVICE_NAME].[NAMESPACE]\n format as seen here:\n\n\n# from within the cluster, pod to pod\nhttp://cm-chartmuseum.charts:8080\n\n# from my browser\nhttp://cm.127.0.0.1.nip.io\n\n\n\n\n\n\nJenkins\n\n\nInstall\n\n\nInstall \nstable/jenkins\n chart with override values (p. 130).\n\n\ncd kube-spec\n\n# set jenkins host env vars\necho $LB_IP\nJENKINS_ADDR=\"jenkins.$LB_IP.nip.io\"\necho $JENKINS_ADDR\n\n# install jenkins (p. 135 and p. 179)\nhelm install stable/jenkins \\\n--name jenkins \\\n--namespace jenkins \\\n--values helm/jenkins-values.yml \\\n--set Master.HostName=$JENKINS_ADDR\n\n# verify\nkubectl -n jenkins rollout status deployment jenkins\n\n\n\n\nopen \nhttp://$JENKINS_ADDR/configure\n (i.e. http://jenkins.127.0.0.1.nip.io) use the \nJENKINS_PASS\n to logon as \nadmin\n.\n\n\nJENKINS_PASS=$(kubectl -n jenkins get secret jenkins \\\n-o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode)\n\necho $JENKINS_PASS\n\n\n\n\nNamespace URLs\n\n\nTo allow communication between the Jenkins master and the slave nodes in other namespaces open \nhttp://$JENKINS_ADDR/configure\n and change the \nhttp://[NAMESPACE]\n format to the \nhttp://[SERVICE_NAME].[NAMESPACE]\n as shown in the following table.\n\n\nCloud->Kubernetes Section:\n\n\n\n\n\n\n\n\nlabel\n\n\nold values\n\n\nupdated value\n\n\n\n\n\n\n\n\n\n\nJenkins URL\n\n\nhttp://jenkins:8080\n\n\nhttp://jenkins.jenkins:8080\n\n\n\n\n\n\nJenkins tunnel\n\n\njenkins-agent:50000\n\n\njenkins-agent.jenkins:50000\n\n\n\n\n\n\n\n\nTiller\n\n\nNote:\n We have Tiller running in the \nkube-system\n Namespace. However, our agent Pods running in \n<app-namespace>\n do not have permissions to access it. We could extend the permissions, but that would allow the pods in that Namespace to gain almost complete control over the whole cluster. Unless your organization is very small, that is often not acceptable. Instead, we\u2019ll deploy another Tiller instance in the \n<app-namespace>\n namespace and tie it to the \nbuild\n ServiceAccount. That will give the new tiller the same permissions in the \n<app-namespace>\n namespace. It\u2019ll be able to do anything in those, but nothing anywhere else (p. 195).\n\n\nhelm init --service-account build --tiller-namespace <app-namespace>\n\n\n\n\nRBAC\n\n\nThis next section I am not so sure about.  Docker-for-windows does not seem to use the rbac, so I will have to validate the following in a real cluster.\n\n\nAdd rbac for jenkins builds.\n\n\nkubectl apply -f k8s/ns.yml\n\n\n\n\nGlobal Pipeline Libraries\n\n\nOpen \nhttp://jenkins.$ADDR/configure\n\n\nSearch for \nGlobal Pipeline Libraries\n section of the configuration, and click the Add button. Type \nmy-library\n as the Name (it can be anything else) and \nmaster\n as the Default version. In our context, the latter defines the branch from which we\u2019ll load the libraries.\n\n\nNext, we\u2019ll click the \nLoad implicitly\n checkbox. As a result, the libraries will be available automatically to all the pipeline jobs. Otherwise, our jobs would need to have @Library('my-library') instruction. Select \nModern SCM\n from the Retrieval method section and select \nGit\n from \nSource Code Management\n and then specify the repository from which Jenkins will load the libraries \nhttps://github.com/tgilkerson/jenkins-shared-libraries.git\n\n\nDon\u2019t forget to click the Save button to persist the changes!\n\n\nGlobal credentials\n\n\nSelect \nCredentials\n and in the \nglobal\n store for the \nJenkins\n scope, then click to \nAdd Credentials\n\n\nDocker Hub\n\n\n scope: Global\n\n Username: tonygilkerson\n\n Password: \nmypwd\n\n\n ID: docker\n\n\nChart Museum\n\n\n scope: Global\n\n Username: admin\n\n Password: admin\n\n ID: chartmuseum\n\n\nUpgrade Jenkins\n\n\nTo upgrade after modifying values in \nhelm/jenkins-values.yml\n (pg126)\n\n\n helm upgrade jenkins stable/jenkins \\\n --values helm/jenkins-values.yml \\\n --set Master.HostName=$HOST \\\n --reuse-values\n\n\n\n\n\n\n\nPrepare Apps\n\n\ngo-demo-5\n\n\ncd go-demo-5\nkubectl apply -f k8s/build.yml\n\n# do once then commit changes\ncat Jenkinsfile.orig \\\n| sed -e \"s@acme.com@$ADDR@g\" \\\n| sed -e \"s@vfarcic@$DH_USER@g\" \\\n| tee Jenkinsfile\n\n# do once then commit changes\ncat helm/go-demo-5/deployment-orig.yaml \\\n| sed -e \"s@vfarcic@$DH_USER@g\" \\\n| tee helm/go-demo-5/templates/deployment.yaml\n\ngit add .\ngit commit -m \"set docker hub user\"\ngit push\n\n\n\n\nInstall Tiller for this app\n\n\nhelm init --service-account build \\\n--tiller-namespace go-demo-5-build\n\n\n\n\nCreate Jenkins Cloud for this App\n\n\nIf you look in the Jenkinsfile you will see a reference to \ngo-demo5-build\n.  \n\n\ncd go-demo-5\ncat Jenkinsfile\n\n...\n  agent {\n    kubernetes {\n      cloud \"go-demo-5-build\"\n      label \"go-demo-5-build\"\n      serviceAccount \"build\"\n      yamlFile \"KubernetesPod.yaml\"\n    }\n...\n\n\n\n\n\nTo configure the cloud open the Jenkins UI and navigate to the \n/configure\n page.\n\n\nPlease scroll to the bottom of the page, expand the \nAdd a new cloud\n list, and select \nKubernetes\n. A new set of fields will appear. Type \ngo-demo-5-build\n as the name. It matches the cloud entry inside kubernetes block of our pipeline. Next, type \ngo-demo-5-build\n as the Kubernetes Namespace. Just as with the other Kubernetes Cloud that was already defined in our Jenkins instance, the value of the Jenkins URL should be \nhttp://prod-jenkins.prod:8080\n, and the Jenkins tunnel should be set to \nprod-jenkins-agent.prod:50000\n.\n\n\nSandbox\n\n\nTo test a release:\n\n\nSANDBOX_ADDR=\"go-demo-5-sandbox.$ADDR\"\n\nhelm install helm/go-demo-5 --name go-demo-5-sandbox --namespace go-demo-5-sandbox\n\n# when done\nhelm delete go-demo-5-sandbox --purge\n\n\n\n\nTODO add verification steps\n\n\n\n\naegjxnode\n\n\ncd aegjxnode\nkubectl apply -f k8s/build.yml\n\n# edit Jenkinsfile and set correct values for the environment section\n...\nenvironment {\n  image = \"tonygilkerson/aegjxnode\"\n  project = \"aegjxnode-build\"\n  domain = \"127.0.0.1.nip.io\"\n  cmAddr = \"cm-chartmuseum.charts:8080\"\n}\n...\n\n# make it stick\ngit add .\ngit commit -m \"set docker hub user\"\ngit push\n\n\n\n\nInstall Tiller for this app\n\n\nhelm init --service-account build \\\n--tiller-namespace aegjxnode-build\n\n\n\n\nCreate Jenkins Cloud for this App\n\n\nIf you look in the Jenkinsfile you will see a reference to \naegjxnode-build\n.  \n\n\ncd aegjxnode\ncat Jenkinsfile\n\n...\n  agent {\n    kubernetes {\n      cloud \"aegjxnode-build\"\n      label \"aegjxnode-build\"\n      serviceAccount \"build\"\n      yamlFile \"KubernetesPod.yaml\"\n    }\n...\n\n\n\n\n\nTo configure the cloud open the Jenkins UI and navigate to the \n/configure\n page.\n\n\nPlease scroll to the bottom of the page, expand the \nAdd a new cloud\n list, and select \nKubernetes\n. A new set of fields will appear. Type \naegjxnode-build\n as the name. It matches the cloud entry inside kubernetes block of our pipeline. Next, type \naegjxnode-build\n as the Kubernetes Namespace. Just as with the other Kubernetes Cloud that was already defined in our Jenkins instance, the value of the Jenkins URL should be \nhttp://prod-jenkins.prod:8080\n, and the Jenkins tunnel should be set to \nprod-jenkins-agent.prod:50000\n.\n\n\nSandbox\n\n\nTo test a release:\n\n\nTODO - refer to step above once they quit changing\n\n\n\n\n\nTODO add verification steps\n\n\n\n\nDeploy Environment\n\n\nOne or more apps are owned by a team.  Each team has a production environment such as \nteam1-pord\n.  The chart in the \nhelm\n folder will reference all the apps that are required for this environment in \nrequirements.yaml\n\n\nManual Deploy\n\n\nClone the environment chart and review the values.\n\n\ngit clone https://github.com/tgilkerson/team1-prod.git\ncd team1-prod\nls -l helm\n\n\n\n\nVerify the host address is correct in \nhelm/values.yaml\n.\n\n\ngo-demo-5:\n  ingress:\n    host: go-demo-5.127.0.0.1.nip.io\naegjxnode:\n  ingress:\n    host: aegjxnode.127.0.0.1.nip.io\n\n\n\n\nLater on, we\u2019ll use Jenkins to install (or upgrade) the Chart, so we should push the changes to GitHub.\n\n\ngit add .\ngit commit -m \"Fix the address\"\ngit push\n\n\n\n\nAll Helm dependencies need to be downloaded to the charts directory before they are installed. We\u2019ll do that through the helm dependency update command. Don\u2019t worry if some of the repositories are not reachable. You might see messages stating that Helm was unable to get an update from \nlocal\n or \nchartmuseum\n repositories.\n\n\nFirst lets do a manually deploy of the \nteam1-prod\n environment.\n\n\ncd team1-prod\n\nhelm dependency update helm\nls helm/charts/\n\n# To confirm\nls -l helm/charts\n\n# The output should look like this\naegjxnode-0.1.9.tgz\ngo-demo-5-0.0.1.tgz\n\n# The first time\nhelm install helm --name team1-prod --namespace team1-prod\n\n# Or do this the -i will install if it does not exist\nhelm upgrade -i team1-prod helm --namespace team1-prod\n\n# verify\nkubectl -n team1-prod rollout status deploy team1-prod-go-demo-5\ncurl go-demo-5.127.0.0.1.nip.io/demo/hello\n\n# clean up\nhelm list\nhelm delete team1-prod --purge\n\n\n\n\n\nAuto Deploy\n\n\nThe CI/CD pipeline will automate the manual steps above.\n\n\n\n\nJeninsX\n\n\nJenkinsX was not in the book, it is too new, however I did look into it a bit to take a look at their workflow.\n\n\nTo install on my docker-for-windows cluster:\n\n\nTaken from \ngetting started\n\n\njx install --provider=kubernetes --on-premise\n\nJenkins X installation completed successfully\nYour admin password is: =rvC9qvz0fq9is^Z6SwY\n\n\n--------------------------------------------------------------------------------\n\n# Utility\n\nSome utility functions and other useful stuff to help debug.\n\n```bash\nkubectl -n <target-namespace> run -it \\\n--image ubuntu aegutil --restart=Never --rm sh\n\n# then you can...\napt-get update\napt-get install curl\napt-get install iputils-ping",
            "title": "Toolkit 2.4 CI/CD"
        },
        {
            "location": "/cicd/cicd-toolkit24/#introduction",
            "text": "This sections contains the notes taken while reading  The DevOps 2.4 Toolkit .  References to page numbers are assumed to be from this book unless otherwise specified.",
            "title": "Introduction"
        },
        {
            "location": "/cicd/cicd-toolkit24/#create-a-cluster",
            "text": "",
            "title": "Create a cluster"
        },
        {
            "location": "/cicd/cicd-toolkit24/#docker-for-windows",
            "text": "Follow  the docker-for-windows doc  to create a cluster on a windows workstation.",
            "title": "Docker for Windows"
        },
        {
            "location": "/cicd/cicd-toolkit24/#kubernetes-dashboard",
            "text": "The full instructions can be found on the  Kubernetes dashboard readme page , in short, run the following:  kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml",
            "title": "Kubernetes dashboard"
        },
        {
            "location": "/cicd/cicd-toolkit24/#accessing-the-dashboard-ui",
            "text": "Run  kubectl proxy  then open  http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/  in your browser.  To log onto the UI you will need a token. Do the following to get a valid token:  SECRET_NAME=$(kubectl -n kube-system \\\nget serviceAccount kubernetes-dashboard \\\n-o  jsonpath='{.secrets[0].name}')\n\nkubectl -n kube-system describe secrets/$SECRET_NAME\n# copy token from the output and use it to log onto the dashboard  For more informatoin on how to configure access see the following:   Accessing the dashboard UI  Configure Service Account",
            "title": "Accessing the dashboard UI"
        },
        {
            "location": "/cicd/cicd-toolkit24/#ingress-install",
            "text": "I got the ingress to work fine on docker-for-win by following the  install instructions for mac , in short, run the following:  kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/cloud-generic.yaml",
            "title": "Ingress Install"
        },
        {
            "location": "/cicd/cicd-toolkit24/#define-some-environment-variables",
            "text": "To make live easy let set some environment variables for our shell:  LB_IP  Find your loadBalancer IP:  On docker-for-windows this is  127.0.0.1   for other cluster you can try the following:  Update  - I think it might be better to use the IP of my laptop i.e.  192.168.1.164 .  This might make is so the pod to pod communication works.  I am not sure.  The later 2.5 book started doing this.  I keep the notes here point to the loopback interface because that is what I used but if I go back through this a second time I should try my laptop IP and if all works out update this doc.  kubectl -n ingress-nginx get svc ingress-nginx -o jsonpath=\"{.status.loadBalancer.ingress[0].hostname}\"\n#or\nkubectl -n ingress-nginx get svc ingress-nginx -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\"\n\n\n# set it\nLB_IP=\"127.0.0.1\"\n\n# or make it stick do this\necho \"export LB_IP=127.0.0.1\" >> ~/.profile\n. ~/.profile  ADDR  ADDR=$LB_IP.nip.io\necho $ADDR\nADDR_ESC=$(echo $ADDR | sed -e \"s@\\.@\\\\\\.@g\")\necho $ADDR_ESC\n\n# or make it stick do this\necho \"export ADDR=$LB_IP.nip.io\" >> ~/.profile\necho 'ADDR_ESC=$(echo $ADDR | sed -e \"s@\\.@\\\\\\.@g\")' >> ~/.profile\n. ~/.profile  DH_USER  Docker Hub user  DH_USER=\"tonygilkerson\"\necho \"export DH_USER=tonygilkerson\" >> ~/.profile\n. ~/.profile\n\necho $DH_USER",
            "title": "Define Some Environment Variables"
        },
        {
            "location": "/cicd/cicd-toolkit24/#client-tools",
            "text": "",
            "title": "Client Tools"
        },
        {
            "location": "/cicd/cicd-toolkit24/#kubectl",
            "text": "Follow the  Install and Setup kubectl  instructions on the official kubernetes site.",
            "title": "kubectl"
        },
        {
            "location": "/cicd/cicd-toolkit24/#helm",
            "text": "Currently helm v2 needs tiller, when we get to v3 tiller will go away.  To install tiller:  cd k8s-spec\n\n# Create service account\nkubectl create \\\n  -f helm/tiller-rbac.yml\n\n# Install server side tiller\nhelm init --service-account tiller\n\n# verify\nkubectl -n kube-system \\\n  rollout status deploy tiller-deploy",
            "title": "helm"
        },
        {
            "location": "/cicd/cicd-toolkit24/#chart-museum",
            "text": "",
            "title": "Chart Museum"
        },
        {
            "location": "/cicd/cicd-toolkit24/#install-chart",
            "text": "Install  stable/chartmuseum  chart with override values (p. 130).  cd kube-spec\n\n# set chartmuseum host env vars\necho $LB_IP\nCM_ADDR=\"cm.$LB_IP.nip.io\"\nCM_ADDR_ESC=$(echo $CM_ADDR | sed -e \"s@\\.@\\\\\\.@g\")\necho $CM_ADDR_ESC\n\n# install chart\nhelm install stable/chartmuseum \\\n--namespace charts \\\n--name cm \\\n--values helm/chartmuseum-values.yml \\\n--set ingress.hosts.\"$CM_ADDR_ESC\"={\"/\"} \\\n--set env.secret.BASIC_AUTH_USER=admin \\\n--set env.secret.BASIC_AUTH_PASS=admin\n\n# verify\nkubectl -n charts \\\nrollout status deploy \\\ncm-chartmuseum\n\ncurl \"http://$CM_ADDR/health\"  \ncurl \"http://$CM_ADDR/index.yaml\" -u admin:admin",
            "title": "Install Chart"
        },
        {
            "location": "/cicd/cicd-toolkit24/#add-to-helm-client",
            "text": "Add the chartmuseum repo to the helm client, then install a plugin that will allow us to push charts to the repo via helm CLI.  helm repo add chartmuseum http://$CM_ADDR --username admin --password admin\nhelm plugin install https://github.com/chartmuseum/helm-push\n\n# example push\nhelm push /path/to/chart chartmuseum --username admin --password admin\n\n# we can also do this\nhelm search chartmuseum/\nhelm repo update\nhelm search chartmuseum/\nhelm inspect chartmuseum/<my-chart>  More useful command can be found at  vfarcic gist",
            "title": "Add to helm client"
        },
        {
            "location": "/cicd/cicd-toolkit24/#upgrade",
            "text": "To upgrade after modifying values.   helm upgrade cm stable/chartmuseum \\\n  --values helm/chartmuseum-values.yml \\\n  --set ingress.hosts.\"$CM_ADDR_ESC\"={\"/\"} \\\n  --set env.secret.BASIC_AUTH_USER=admin \\\n  --set env.secret.BASIC_AUTH_PASS=admin",
            "title": "Upgrade"
        },
        {
            "location": "/cicd/cicd-toolkit24/#accessing-the-ui",
            "text": "To access chartmuseum from my laptop I use  $CM_ADDR  as seen above however this is not valid in a pipeline build container. From within a container use the  http://[SERVICE_NAME].[NAMESPACE]  format as seen here:  # from within the cluster, pod to pod\nhttp://cm-chartmuseum.charts:8080\n\n# from my browser\nhttp://cm.127.0.0.1.nip.io",
            "title": "Accessing the UI"
        },
        {
            "location": "/cicd/cicd-toolkit24/#jenkins",
            "text": "",
            "title": "Jenkins"
        },
        {
            "location": "/cicd/cicd-toolkit24/#install",
            "text": "Install  stable/jenkins  chart with override values (p. 130).  cd kube-spec\n\n# set jenkins host env vars\necho $LB_IP\nJENKINS_ADDR=\"jenkins.$LB_IP.nip.io\"\necho $JENKINS_ADDR\n\n# install jenkins (p. 135 and p. 179)\nhelm install stable/jenkins \\\n--name jenkins \\\n--namespace jenkins \\\n--values helm/jenkins-values.yml \\\n--set Master.HostName=$JENKINS_ADDR\n\n# verify\nkubectl -n jenkins rollout status deployment jenkins  open  http://$JENKINS_ADDR/configure  (i.e. http://jenkins.127.0.0.1.nip.io) use the  JENKINS_PASS  to logon as  admin .  JENKINS_PASS=$(kubectl -n jenkins get secret jenkins \\\n-o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode)\n\necho $JENKINS_PASS",
            "title": "Install"
        },
        {
            "location": "/cicd/cicd-toolkit24/#namespace-urls",
            "text": "To allow communication between the Jenkins master and the slave nodes in other namespaces open  http://$JENKINS_ADDR/configure  and change the  http://[NAMESPACE]  format to the  http://[SERVICE_NAME].[NAMESPACE]  as shown in the following table.  Cloud->Kubernetes Section:     label  old values  updated value      Jenkins URL  http://jenkins:8080  http://jenkins.jenkins:8080    Jenkins tunnel  jenkins-agent:50000  jenkins-agent.jenkins:50000",
            "title": "Namespace URLs"
        },
        {
            "location": "/cicd/cicd-toolkit24/#tiller",
            "text": "Note:  We have Tiller running in the  kube-system  Namespace. However, our agent Pods running in  <app-namespace>  do not have permissions to access it. We could extend the permissions, but that would allow the pods in that Namespace to gain almost complete control over the whole cluster. Unless your organization is very small, that is often not acceptable. Instead, we\u2019ll deploy another Tiller instance in the  <app-namespace>  namespace and tie it to the  build  ServiceAccount. That will give the new tiller the same permissions in the  <app-namespace>  namespace. It\u2019ll be able to do anything in those, but nothing anywhere else (p. 195).  helm init --service-account build --tiller-namespace <app-namespace>",
            "title": "Tiller"
        },
        {
            "location": "/cicd/cicd-toolkit24/#rbac",
            "text": "This next section I am not so sure about.  Docker-for-windows does not seem to use the rbac, so I will have to validate the following in a real cluster.  Add rbac for jenkins builds.  kubectl apply -f k8s/ns.yml",
            "title": "RBAC"
        },
        {
            "location": "/cicd/cicd-toolkit24/#global-pipeline-libraries",
            "text": "Open  http://jenkins.$ADDR/configure  Search for  Global Pipeline Libraries  section of the configuration, and click the Add button. Type  my-library  as the Name (it can be anything else) and  master  as the Default version. In our context, the latter defines the branch from which we\u2019ll load the libraries.  Next, we\u2019ll click the  Load implicitly  checkbox. As a result, the libraries will be available automatically to all the pipeline jobs. Otherwise, our jobs would need to have @Library('my-library') instruction. Select  Modern SCM  from the Retrieval method section and select  Git  from  Source Code Management  and then specify the repository from which Jenkins will load the libraries  https://github.com/tgilkerson/jenkins-shared-libraries.git  Don\u2019t forget to click the Save button to persist the changes!",
            "title": "Global Pipeline Libraries"
        },
        {
            "location": "/cicd/cicd-toolkit24/#global-credentials",
            "text": "Select  Credentials  and in the  global  store for the  Jenkins  scope, then click to  Add Credentials  Docker Hub   scope: Global  Username: tonygilkerson  Password:  mypwd   ID: docker  Chart Museum   scope: Global  Username: admin  Password: admin  ID: chartmuseum",
            "title": "Global credentials"
        },
        {
            "location": "/cicd/cicd-toolkit24/#upgrade-jenkins",
            "text": "To upgrade after modifying values in  helm/jenkins-values.yml  (pg126)   helm upgrade jenkins stable/jenkins \\\n --values helm/jenkins-values.yml \\\n --set Master.HostName=$HOST \\\n --reuse-values",
            "title": "Upgrade Jenkins"
        },
        {
            "location": "/cicd/cicd-toolkit24/#prepare-apps",
            "text": "",
            "title": "Prepare Apps"
        },
        {
            "location": "/cicd/cicd-toolkit24/#go-demo-5",
            "text": "cd go-demo-5\nkubectl apply -f k8s/build.yml\n\n# do once then commit changes\ncat Jenkinsfile.orig \\\n| sed -e \"s@acme.com@$ADDR@g\" \\\n| sed -e \"s@vfarcic@$DH_USER@g\" \\\n| tee Jenkinsfile\n\n# do once then commit changes\ncat helm/go-demo-5/deployment-orig.yaml \\\n| sed -e \"s@vfarcic@$DH_USER@g\" \\\n| tee helm/go-demo-5/templates/deployment.yaml\n\ngit add .\ngit commit -m \"set docker hub user\"\ngit push  Install Tiller for this app  helm init --service-account build \\\n--tiller-namespace go-demo-5-build  Create Jenkins Cloud for this App  If you look in the Jenkinsfile you will see a reference to  go-demo5-build .    cd go-demo-5\ncat Jenkinsfile\n\n...\n  agent {\n    kubernetes {\n      cloud \"go-demo-5-build\"\n      label \"go-demo-5-build\"\n      serviceAccount \"build\"\n      yamlFile \"KubernetesPod.yaml\"\n    }\n...  To configure the cloud open the Jenkins UI and navigate to the  /configure  page.  Please scroll to the bottom of the page, expand the  Add a new cloud  list, and select  Kubernetes . A new set of fields will appear. Type  go-demo-5-build  as the name. It matches the cloud entry inside kubernetes block of our pipeline. Next, type  go-demo-5-build  as the Kubernetes Namespace. Just as with the other Kubernetes Cloud that was already defined in our Jenkins instance, the value of the Jenkins URL should be  http://prod-jenkins.prod:8080 , and the Jenkins tunnel should be set to  prod-jenkins-agent.prod:50000 .  Sandbox  To test a release:  SANDBOX_ADDR=\"go-demo-5-sandbox.$ADDR\"\n\nhelm install helm/go-demo-5 --name go-demo-5-sandbox --namespace go-demo-5-sandbox\n\n# when done\nhelm delete go-demo-5-sandbox --purge  TODO add verification steps",
            "title": "go-demo-5"
        },
        {
            "location": "/cicd/cicd-toolkit24/#aegjxnode",
            "text": "cd aegjxnode\nkubectl apply -f k8s/build.yml\n\n# edit Jenkinsfile and set correct values for the environment section\n...\nenvironment {\n  image = \"tonygilkerson/aegjxnode\"\n  project = \"aegjxnode-build\"\n  domain = \"127.0.0.1.nip.io\"\n  cmAddr = \"cm-chartmuseum.charts:8080\"\n}\n...\n\n# make it stick\ngit add .\ngit commit -m \"set docker hub user\"\ngit push  Install Tiller for this app  helm init --service-account build \\\n--tiller-namespace aegjxnode-build  Create Jenkins Cloud for this App  If you look in the Jenkinsfile you will see a reference to  aegjxnode-build .    cd aegjxnode\ncat Jenkinsfile\n\n...\n  agent {\n    kubernetes {\n      cloud \"aegjxnode-build\"\n      label \"aegjxnode-build\"\n      serviceAccount \"build\"\n      yamlFile \"KubernetesPod.yaml\"\n    }\n...  To configure the cloud open the Jenkins UI and navigate to the  /configure  page.  Please scroll to the bottom of the page, expand the  Add a new cloud  list, and select  Kubernetes . A new set of fields will appear. Type  aegjxnode-build  as the name. It matches the cloud entry inside kubernetes block of our pipeline. Next, type  aegjxnode-build  as the Kubernetes Namespace. Just as with the other Kubernetes Cloud that was already defined in our Jenkins instance, the value of the Jenkins URL should be  http://prod-jenkins.prod:8080 , and the Jenkins tunnel should be set to  prod-jenkins-agent.prod:50000 .  Sandbox  To test a release:  TODO - refer to step above once they quit changing  TODO add verification steps",
            "title": "aegjxnode"
        },
        {
            "location": "/cicd/cicd-toolkit24/#deploy-environment",
            "text": "One or more apps are owned by a team.  Each team has a production environment such as  team1-pord .  The chart in the  helm  folder will reference all the apps that are required for this environment in  requirements.yaml",
            "title": "Deploy Environment"
        },
        {
            "location": "/cicd/cicd-toolkit24/#manual-deploy",
            "text": "Clone the environment chart and review the values.  git clone https://github.com/tgilkerson/team1-prod.git\ncd team1-prod\nls -l helm  Verify the host address is correct in  helm/values.yaml .  go-demo-5:\n  ingress:\n    host: go-demo-5.127.0.0.1.nip.io\naegjxnode:\n  ingress:\n    host: aegjxnode.127.0.0.1.nip.io  Later on, we\u2019ll use Jenkins to install (or upgrade) the Chart, so we should push the changes to GitHub.  git add .\ngit commit -m \"Fix the address\"\ngit push  All Helm dependencies need to be downloaded to the charts directory before they are installed. We\u2019ll do that through the helm dependency update command. Don\u2019t worry if some of the repositories are not reachable. You might see messages stating that Helm was unable to get an update from  local  or  chartmuseum  repositories.  First lets do a manually deploy of the  team1-prod  environment.  cd team1-prod\n\nhelm dependency update helm\nls helm/charts/\n\n# To confirm\nls -l helm/charts\n\n# The output should look like this\naegjxnode-0.1.9.tgz\ngo-demo-5-0.0.1.tgz\n\n# The first time\nhelm install helm --name team1-prod --namespace team1-prod\n\n# Or do this the -i will install if it does not exist\nhelm upgrade -i team1-prod helm --namespace team1-prod\n\n# verify\nkubectl -n team1-prod rollout status deploy team1-prod-go-demo-5\ncurl go-demo-5.127.0.0.1.nip.io/demo/hello\n\n# clean up\nhelm list\nhelm delete team1-prod --purge  Auto Deploy  The CI/CD pipeline will automate the manual steps above.",
            "title": "Manual Deploy"
        },
        {
            "location": "/cicd/cicd-toolkit24/#jeninsx",
            "text": "JenkinsX was not in the book, it is too new, however I did look into it a bit to take a look at their workflow.  To install on my docker-for-windows cluster:  Taken from  getting started  jx install --provider=kubernetes --on-premise\n\nJenkins X installation completed successfully\nYour admin password is: =rvC9qvz0fq9is^Z6SwY\n\n\n--------------------------------------------------------------------------------\n\n# Utility\n\nSome utility functions and other useful stuff to help debug.\n\n```bash\nkubectl -n <target-namespace> run -it \\\n--image ubuntu aegutil --restart=Never --rm sh\n\n# then you can...\napt-get update\napt-get install curl\napt-get install iputils-ping",
            "title": "JeninsX"
        },
        {
            "location": "/cicd/jx-toolkit26/",
            "text": "Create Cluster\n\n\nNotes taken form \nViktor Farcic. The DevOps 2.6 Toolkit: Jenkins X leanpub.com\n\n\nCommands for this section: \n02-intro.sh\n\n\nInstall prerequisites tools\n\n\nSee \"Manually install prerequisites (Kindle Location 99)\" for more detail:\n\n\n\n\ngit (and gitbash)\n\n\nkubectl\n\n\nHelm\n\n\nAWS CLI and eksctl (for  EKS)\n\n\ngcloud (for GKE)\n\n\nAzure CLI (for AKS)\n\n\njq\n\n\nhub\n\n\njenkins-x\n\n\n\n\nGitBash tweak\n\n\nTo get the \ngcloud\n command to work in gitbash I had to add python to the path.  If this does not work you can try just running the command in PS.\n\n\ncd\necho \"export PATH=$PATH:/c/Python27\" >> .bashrc\n# open a new shell\n\n\n\n\nCreate Cluster with jx\n\n\nDisable Nexus\n\n\nI will not need Nexus most of the time so just turn it off.\n\n\necho \"nexus:\n  enabled: false\" | tee myvalues.yaml\n\n\n\n\nCreate the cluster\n\n\n# If GKE\nPROJECT=\"aeg-jenkinsx\"\nJX_ENV=\"jxeMMDD\" # this changes each time\n\n# If GKE (note winpty is needed for gitbash)\nwinpty jx create cluster gke \\\n    -n $JX_ENV \\\n    -p $PROJECT \\\n    -z us-east1-b \\\n    -m n1-standard-1 \\\n    --min-num-nodes 2 \\\n    --max-num-nodes 5 \\\n    --default-admin-password aegadmin \\\n    --default-environment-prefix $JX_ENV \\\n    --preemptible=true # why not I am just testing\n\n\n\n\nDestroy Cluster\n\n\nRemove the cluster and everything else.\n\n\nGH_USER=\"tonygilkerson\"\n\nhub delete -y $GH_USER/environment-$JX_ENV-staging\nhub delete -y $GH_USER/environment-$JX_ENV-production\n\nrm -rf ~/.jx/environments/$GH_USER/environment-$JX_ENV-*\nrm -f ~/.jx/jenkinsAuth.yaml\n\n# for gcp\n# Note for gitbash I needed to use winpty and add '.cmd' to the end of gcloud\nwinpty gcloud.cmd container clusters delete $JX_ENV --zone us-east1-b\n\n\n# remove unused disks\nwinpty gcloud.cmd compute disks delete \\\n    $(gcloud compute disks list \\\n    --filter=\"-users:*\" \\\n    --format=\"value(id)\")\n\n\n\n\nExploring Quickstart Projects\n\n\nCreate a sample project to see how things work.\n\n\nCommands for this section: \n03-quickstart.sh\n\n\nPrerequisites\n\n\nCreate a cluster as we did before, see \nCreate Cluster\n\n\nCreate project\n\n\nQS_PROJECT=\"jxqsMMDD\" # change this as needed\n\nwinpty jx create quickstart -l go -p $QS_PROJECT -b\n\n\n\n\nCleanup project\n\n\nDestroy Cluster\n as we did before.  Then cleanup the quickstart app:\n\n\nhub delete -y $GH_USER/$QS_PROJECT\n\n# TODO need to verify\ncd ..\nrm -rf $QS_PROJECT",
            "title": "Toolkit 2.6 JenkinX"
        },
        {
            "location": "/cicd/jx-toolkit26/#create-cluster",
            "text": "Notes taken form  Viktor Farcic. The DevOps 2.6 Toolkit: Jenkins X leanpub.com  Commands for this section:  02-intro.sh",
            "title": "Create Cluster"
        },
        {
            "location": "/cicd/jx-toolkit26/#install-prerequisites-tools",
            "text": "See \"Manually install prerequisites (Kindle Location 99)\" for more detail:   git (and gitbash)  kubectl  Helm  AWS CLI and eksctl (for  EKS)  gcloud (for GKE)  Azure CLI (for AKS)  jq  hub  jenkins-x   GitBash tweak  To get the  gcloud  command to work in gitbash I had to add python to the path.  If this does not work you can try just running the command in PS.  cd\necho \"export PATH=$PATH:/c/Python27\" >> .bashrc\n# open a new shell",
            "title": "Install prerequisites tools"
        },
        {
            "location": "/cicd/jx-toolkit26/#create-cluster-with-jx",
            "text": "Disable Nexus  I will not need Nexus most of the time so just turn it off.  echo \"nexus:\n  enabled: false\" | tee myvalues.yaml  Create the cluster  # If GKE\nPROJECT=\"aeg-jenkinsx\"\nJX_ENV=\"jxeMMDD\" # this changes each time\n\n# If GKE (note winpty is needed for gitbash)\nwinpty jx create cluster gke \\\n    -n $JX_ENV \\\n    -p $PROJECT \\\n    -z us-east1-b \\\n    -m n1-standard-1 \\\n    --min-num-nodes 2 \\\n    --max-num-nodes 5 \\\n    --default-admin-password aegadmin \\\n    --default-environment-prefix $JX_ENV \\\n    --preemptible=true # why not I am just testing",
            "title": "Create Cluster with jx"
        },
        {
            "location": "/cicd/jx-toolkit26/#destroy-cluster",
            "text": "Remove the cluster and everything else.  GH_USER=\"tonygilkerson\"\n\nhub delete -y $GH_USER/environment-$JX_ENV-staging\nhub delete -y $GH_USER/environment-$JX_ENV-production\n\nrm -rf ~/.jx/environments/$GH_USER/environment-$JX_ENV-*\nrm -f ~/.jx/jenkinsAuth.yaml\n\n# for gcp\n# Note for gitbash I needed to use winpty and add '.cmd' to the end of gcloud\nwinpty gcloud.cmd container clusters delete $JX_ENV --zone us-east1-b\n\n\n# remove unused disks\nwinpty gcloud.cmd compute disks delete \\\n    $(gcloud compute disks list \\\n    --filter=\"-users:*\" \\\n    --format=\"value(id)\")",
            "title": "Destroy Cluster"
        },
        {
            "location": "/cicd/jx-toolkit26/#exploring-quickstart-projects",
            "text": "Create a sample project to see how things work.  Commands for this section:  03-quickstart.sh  Prerequisites  Create a cluster as we did before, see  Create Cluster",
            "title": "Exploring Quickstart Projects"
        },
        {
            "location": "/cicd/jx-toolkit26/#create-project",
            "text": "QS_PROJECT=\"jxqsMMDD\" # change this as needed\n\nwinpty jx create quickstart -l go -p $QS_PROJECT -b",
            "title": "Create project"
        },
        {
            "location": "/cicd/jx-toolkit26/#cleanup-project",
            "text": "Destroy Cluster  as we did before.  Then cleanup the quickstart app:  hub delete -y $GH_USER/$QS_PROJECT\n\n# TODO need to verify\ncd ..\nrm -rf $QS_PROJECT",
            "title": "Cleanup project"
        },
        {
            "location": "/cicd/jx-accelerate-deck/",
            "text": "Accelerate Deck\n\n\nThis section contains the notes taken while watching the  \nAccelerate your CI/CD on Kubernetes with Jenkins X (James Strachan)\n\n\nJames Strachan recommends this book and says Jenkins X tries to adopt many of the principles in this book: \nAccelerate: The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organizations\n\n\n\n\n7 Capabilities of Jenkins X\n\n\nTaken from the book here are 7 practices of high performing teams. Jenkins X uses open source software to automate these practices.   \n\n\n1. Use version control for all artifacts\n\n\n\n\nsource code and code (IaC)\n\n\nwhen things go bad we can revert\n\n\nprovides an audit trail\n\n\n\n\n2. Use trunk-based development\n\n\n\n\ndevelopers collaborate on code in a single branch called 'trunk'. They therefore avoid merge hell, do not break the build, and live happily ever after.\n\n\nthe use of long-lived featured branches is associated with low performing teams\n\n\ntrunk-based development is associated with high performing teams\n\n\n\n\n6. Use loosely coupled architecture\n\n\n\n\nHigh performing teams use the cloud well  to deliver highly available, multi-az deployments that are self healing and auto scaling.\n\n\nmicroservices allows teams to move quicker; many \nindependent\n teams can move quicker than one large team.\n\n\nKubernetes is ideal for running microservices.  \n\n\nruns everywhere while providing a consistent abstraction across all cloud providers\n\n\na single way to package applications and run them in any cloud (or on premise)!\n\n\nmulti cloud is now achievable by mortals\n\n\n\n\n\n\nchallenge: teams now need to figure out how to do many things well; microservices, cloud, Kubernetes, CI/CD, etc.. this is where Jenkins X comes in\n\n\n\n\n3. Implement continuous integration (CI)\n\n\n4. Implement continuous delivery (CD)\n\n\n5. Automate your deployment process\n\n\n7. Architect for empowered teams\n\n\n\n\nHow does Jenkins X help?\n\n\nAutomates the setup of your tools +environments\n\n\n\n\nJenkins, helm, skaffold, nexus, monocular\n\n\n\n\nAutomates the CI/CD for your application on Kubernetes\n\n\n\n\nDocker images\n\n\nHelm charts\n\n\nJenkins Pipelines\n\n\n\n\nUses GitOps to manage promotion between environments\n\n\n\n\nTest -> Staging -> Production\n\n\nuse git as the source of truth for each environment;\n\n\ne.g. a prod repo will list all the microservices, the version of each and any environment specific configuration running in the prod environment\n\n\ncould use \nkubectl\n to promote code to various environments but no one can see what you are doing, reverting back is not as easy and it is dangerous especially when you are trying to move fast.\n\n\nto promote between environments we use a PR. The team can code review/approve coding as well as configuration changes.\n\n\nAn approved PR tells the team what is in prod and when it was installed.  And, if things go bad, just revert the PR to roll back.\n\n\n\n\nLots of feedback\n\n\n\n\nE.g. commenting on issues as they hit Staging + Production\n\n\n\n\n\n\nInstalling Jenkins X\n\n\nInstall the jx binary\n\n\nhttps://jenkins-x.io/geting-started/install/\n\n\n\n\nCreatea new k8scluster on GKE\n\n\n$ jx create cluster gke\n\n\n\n\nAs of now, supported clouds:\n\n\n\n\ncreate cluster aks Create a new Kubernetes cluster on AKS: Runs on Azure\n\n\ncreate cluster aws Create a new Kubernetes cluster on AWS with kops\n\n\ncreate cluster eks Create a new Kubernetes cluster on AWS using EKS\n\n\ncreate cluster gke Create a new Kubernetes cluster on GKE: Runs on Google Cloud\n\n\ncreate cluster iks Create a new kubernetes cluster on IBM Cloud Kubernetes Services\n\n\ncreate cluster minikube Create a new Kubernetes cluster with Minikube: Runs locally\n\n\ncreate cluster minishift Create a new OpenShift cluster with Minishift: Runs locally\n\n\ncreate cluster oke Create a new Kubernetes cluster on OKE: Runs on Oracle Cloud\n\n\n\n\nInstall Jenkins X on an existing cluster\n\n\n$ jx install --provider=...\n\n\n\n\n\n\nWhat does that give me?\n\n\nEach team gets their own:\n\n\n\n\nDevelopment tools environment\n\n\nJenkins master\n\n\nElastic pool of Kubernetes build pods\n\n\nNexus + Monocular (helm application store)\n\n\n\n\n\n\nStaging environment\n\n\nProduction environment\n\n\n\n\nAlso...\n\n\n\n\nThis empowers each team to work independently and release when they want\n\n\nCan configure more environments but out-of-the-box you get one staging and one production environment.\n\n\nA new version is released for each PR merged to 'trunk'\n\n\nEach release is automatically promoted to staging but requires a manual step to promote to production by default.  This can be changed for 100% automation.\n\n\n\n\n\n\nImporting and Creating Projects\n\n\nImport existing\n\n\nIf you already have some code.\n\n\n$ jx import\n\n\n\n\nCreate new applications from quickstarts\n\n\nThis is preferred so you know everything is setup correctly. The import works but you never know.\n\n\n$ jx create quickstart\n\n\n\n\n\n\nthere are a bunch of quick starts they are open source so you can check them out \nhere\n\n\nThere is one for most of the popular development platforms\n\n\nYou can create a custom template if you like\n\n\n\n\nCreate new Spring Boot applications\n\n\n$ jx create spring\n\n\n\n\nWhat you get from a quickstart\n\n\njx create\n will...\n\n\n\n\nCreate a project repo GitHub\n\n\nCreate a dockerfile\n\n\nCreate a helm chart\n\n\nCreate environment repos in GitHub (staging and production)\n\n\nSetup all the webhooks in GitHub\n\n\nSetup all your CI/CD (Jenkins pipelines)",
            "title": "JenkinsX Accelerate Deck"
        },
        {
            "location": "/cicd/jx-accelerate-deck/#accelerate-deck",
            "text": "This section contains the notes taken while watching the   Accelerate your CI/CD on Kubernetes with Jenkins X (James Strachan)  James Strachan recommends this book and says Jenkins X tries to adopt many of the principles in this book:  Accelerate: The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organizations",
            "title": "Accelerate Deck"
        },
        {
            "location": "/cicd/jx-accelerate-deck/#7-capabilities-of-jenkins-x",
            "text": "Taken from the book here are 7 practices of high performing teams. Jenkins X uses open source software to automate these practices.     1. Use version control for all artifacts   source code and code (IaC)  when things go bad we can revert  provides an audit trail   2. Use trunk-based development   developers collaborate on code in a single branch called 'trunk'. They therefore avoid merge hell, do not break the build, and live happily ever after.  the use of long-lived featured branches is associated with low performing teams  trunk-based development is associated with high performing teams   6. Use loosely coupled architecture   High performing teams use the cloud well  to deliver highly available, multi-az deployments that are self healing and auto scaling.  microservices allows teams to move quicker; many  independent  teams can move quicker than one large team.  Kubernetes is ideal for running microservices.    runs everywhere while providing a consistent abstraction across all cloud providers  a single way to package applications and run them in any cloud (or on premise)!  multi cloud is now achievable by mortals    challenge: teams now need to figure out how to do many things well; microservices, cloud, Kubernetes, CI/CD, etc.. this is where Jenkins X comes in   3. Implement continuous integration (CI)  4. Implement continuous delivery (CD)  5. Automate your deployment process  7. Architect for empowered teams",
            "title": "7 Capabilities of Jenkins X"
        },
        {
            "location": "/cicd/jx-accelerate-deck/#how-does-jenkins-x-help",
            "text": "Automates the setup of your tools +environments   Jenkins, helm, skaffold, nexus, monocular   Automates the CI/CD for your application on Kubernetes   Docker images  Helm charts  Jenkins Pipelines   Uses GitOps to manage promotion between environments   Test -> Staging -> Production  use git as the source of truth for each environment;  e.g. a prod repo will list all the microservices, the version of each and any environment specific configuration running in the prod environment  could use  kubectl  to promote code to various environments but no one can see what you are doing, reverting back is not as easy and it is dangerous especially when you are trying to move fast.  to promote between environments we use a PR. The team can code review/approve coding as well as configuration changes.  An approved PR tells the team what is in prod and when it was installed.  And, if things go bad, just revert the PR to roll back.   Lots of feedback   E.g. commenting on issues as they hit Staging + Production",
            "title": "How does Jenkins X help?"
        },
        {
            "location": "/cicd/jx-accelerate-deck/#installing-jenkins-x",
            "text": "Install the jx binary  https://jenkins-x.io/geting-started/install/  Createa new k8scluster on GKE  $ jx create cluster gke  As of now, supported clouds:   create cluster aks Create a new Kubernetes cluster on AKS: Runs on Azure  create cluster aws Create a new Kubernetes cluster on AWS with kops  create cluster eks Create a new Kubernetes cluster on AWS using EKS  create cluster gke Create a new Kubernetes cluster on GKE: Runs on Google Cloud  create cluster iks Create a new kubernetes cluster on IBM Cloud Kubernetes Services  create cluster minikube Create a new Kubernetes cluster with Minikube: Runs locally  create cluster minishift Create a new OpenShift cluster with Minishift: Runs locally  create cluster oke Create a new Kubernetes cluster on OKE: Runs on Oracle Cloud   Install Jenkins X on an existing cluster  $ jx install --provider=...",
            "title": "Installing Jenkins X"
        },
        {
            "location": "/cicd/jx-accelerate-deck/#what-does-that-give-me",
            "text": "Each team gets their own:   Development tools environment  Jenkins master  Elastic pool of Kubernetes build pods  Nexus + Monocular (helm application store)    Staging environment  Production environment   Also...   This empowers each team to work independently and release when they want  Can configure more environments but out-of-the-box you get one staging and one production environment.  A new version is released for each PR merged to 'trunk'  Each release is automatically promoted to staging but requires a manual step to promote to production by default.  This can be changed for 100% automation.",
            "title": "What does that give me?"
        },
        {
            "location": "/cicd/jx-accelerate-deck/#importing-and-creating-projects",
            "text": "Import existing  If you already have some code.  $ jx import  Create new applications from quickstarts  This is preferred so you know everything is setup correctly. The import works but you never know.  $ jx create quickstart   there are a bunch of quick starts they are open source so you can check them out  here  There is one for most of the popular development platforms  You can create a custom template if you like   Create new Spring Boot applications  $ jx create spring",
            "title": "Importing and Creating Projects"
        },
        {
            "location": "/cicd/jx-accelerate-deck/#what-you-get-from-a-quickstart",
            "text": "jx create  will...   Create a project repo GitHub  Create a dockerfile  Create a helm chart  Create environment repos in GitHub (staging and production)  Setup all the webhooks in GitHub  Setup all your CI/CD (Jenkins pipelines)",
            "title": "What you get from a quickstart"
        },
        {
            "location": "/about/",
            "text": "This is my personal notes Ops related stuff.",
            "title": "About"
        },
        {
            "location": "/ref/",
            "text": "Kubernetes\n\n\nDocker Desktop Dashboard\n\n\nInstall dashboard\n\n\n#install\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml\n\n#open\nkubectl proxy\nopen http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/\n\n\n\n\nlogon\n\n\nDASH_SECRET=$(kubectl get serviceAccounts -o jsonpath=\"{.items[0].secrets[0].name}\")\n\n# cut/past the token shown form this command\nkubectl describe secrets/$DASH_SECRET\n\n\n\n\ncharts and graphs\n\n\nThis stuff is optional but will make charts and graphs show up in the dashboard.\n\n\nkubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml\nkubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml\nkubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml\n\n\n\n\nHeapster workaround:\n\n\nI got an error where heapster could not connect so I had to edit the \n--source\n value for the deployment as shown below\n\n\n{\n  \"kind\": \"Deployment\",\n  \"apiVersion\": \"extensions/v1beta1\",\n  \"metadata\": {\n    \"name\": \"heapster\",\n    \"namespace\": \"kube-system\",\n    ...,\n        \"spec\": {\n        \"containers\": [\n          {\n            ...\n            \"command\": [\n              \"/heapster\",\n====>         \"--source=kubernetes:https://kubernetes.default:443?useServiceAccount=true&kubeletHttps=true&kubeletPort=10250&insecure=true\",\n              \"--sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\"\n            ],\n            ...\n    ...\n }\n\n\n\n\nFor more information see:\n\n\nSetup:\n\n\n\n\nhttps://github.com/kubernetes/dashboard\n\n\nthis blog post\n\n\n\n\nConfigure access:\n\n\n\n\nhttps://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#accessing-the-dashboard-ui\n\n\nhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/  \n\n\n\n\nKubernetes API Doc\n\n\n\n\nKubernetes API\n\n\n\n\nBash scripting cheatsheet\n\n\n\n\nDev Hints\n\n\n\n\nThe DevOps 2.3 Toolkit: Kubernetes\n\n\n\n\n02-minikube.sh\n\n\n03-pod.hs\n\n\n04-rs.sh\n\n\n05-svc.sh\n\n\n06-deploy.sh\n\n\n07-ingress.sh\n\n\n08-volume.sh\n\n\n09-config-map.sh\n\n\n10-secret.sh\n\n\n11-ns.sh\n\n\n12-auth.sh\n\n\n13-resource.sh\n\n\n14-aws.sh\n\n\n15-pv.sh",
            "title": "Reference"
        },
        {
            "location": "/ref/#kubernetes",
            "text": "",
            "title": "Kubernetes"
        },
        {
            "location": "/ref/#docker-desktop-dashboard",
            "text": "Install dashboard  #install\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml\n\n#open\nkubectl proxy\nopen http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/  logon  DASH_SECRET=$(kubectl get serviceAccounts -o jsonpath=\"{.items[0].secrets[0].name}\")\n\n# cut/past the token shown form this command\nkubectl describe secrets/$DASH_SECRET  charts and graphs  This stuff is optional but will make charts and graphs show up in the dashboard.  kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml\nkubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml\nkubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml  Heapster workaround:  I got an error where heapster could not connect so I had to edit the  --source  value for the deployment as shown below  {\n  \"kind\": \"Deployment\",\n  \"apiVersion\": \"extensions/v1beta1\",\n  \"metadata\": {\n    \"name\": \"heapster\",\n    \"namespace\": \"kube-system\",\n    ...,\n        \"spec\": {\n        \"containers\": [\n          {\n            ...\n            \"command\": [\n              \"/heapster\",\n====>         \"--source=kubernetes:https://kubernetes.default:443?useServiceAccount=true&kubeletHttps=true&kubeletPort=10250&insecure=true\",\n              \"--sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\"\n            ],\n            ...\n    ...\n }  For more information see:  Setup:   https://github.com/kubernetes/dashboard  this blog post   Configure access:   https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#accessing-the-dashboard-ui  https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/",
            "title": "Docker Desktop Dashboard"
        },
        {
            "location": "/ref/#kubernetes-api-doc",
            "text": "Kubernetes API",
            "title": "Kubernetes API Doc"
        },
        {
            "location": "/ref/#bash-scripting-cheatsheet",
            "text": "Dev Hints",
            "title": "Bash scripting cheatsheet"
        },
        {
            "location": "/ref/#the-devops-23-toolkit-kubernetes",
            "text": "02-minikube.sh  03-pod.hs  04-rs.sh  05-svc.sh  06-deploy.sh  07-ingress.sh  08-volume.sh  09-config-map.sh  10-secret.sh  11-ns.sh  12-auth.sh  13-resource.sh  14-aws.sh  15-pv.sh",
            "title": "The DevOps 2.3 Toolkit: Kubernetes"
        }
    ]
}